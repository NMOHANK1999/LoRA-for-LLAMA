{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LoRALinear(nn.Linear):\n",
    "    def __init__(self,\n",
    "                 # nn.Linear parameters\n",
    "                 in_features: int,\n",
    "                 out_features: int,\n",
    "                 bias: bool = True,\n",
    "                 device=None,\n",
    "                 dtype=None,\n",
    "                 # LoRA parameters\n",
    "                 lora_rank: int = 0,\n",
    "                 lora_alpha: float = 0.0,\n",
    "                 lora_dropout: float = 0.0,\n",
    "                ) -> None:\n",
    "        \n",
    "        # Initialize the inherited class, nn.Linear\n",
    "        super(LoRALinear, self).__init__(in_features, out_features, bias, device=device, dtype=dtype)\n",
    "\n",
    "        self.has_weights_merged = False\n",
    "        if lora_rank > 0:\n",
    "            self.lora_dropout = nn.Dropout(lora_dropout)\n",
    "            self.lora_scaling = lora_alpha / lora_rank\n",
    "\n",
    "            # Define the LoRA matrices A and B\n",
    "            self.lora_A = nn.Linear(in_features, lora_rank, bias=bias)\n",
    "            self.lora_B = nn.Linear(lora_rank, out_features, bias=bias)\n",
    "            \n",
    "            # Make sure LoRA matrices don't get updated during training\n",
    "            self.lora_A.weight.requires_grad = False\n",
    "            self.lora_B.weight.requires_grad = False\n",
    "\n",
    "            self.reset_parameters()\n",
    "\n",
    "    def is_lora(self) -> bool:\n",
    "        return hasattr(self, 'lora_A')\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        nn.Linear.reset_parameters(self)\n",
    "        if self.is_lora():\n",
    "            # Initialize lora_A with kaiming_uniform_ using negative slope as math.sqrt(5)\n",
    "            nn.init.kaiming_uniform_(self.lora_A.weight, a=math.sqrt(5))\n",
    "            # Initialize lora_B weights to zero\n",
    "            nn.init.zeros_(self.lora_B.weight)\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        if self.is_lora():\n",
    "            if not self.has_weights_merged:\n",
    "                # Apply the LoRA adaptation to the input\n",
    "                lora_out = self.lora_B(self.lora_A(input))\n",
    "                lora_out = self.lora_dropout(lora_out) * self.lora_scaling\n",
    "                # Add LoRA to the original nn.Linear output\n",
    "                return super().forward(input) + lora_out\n",
    "        return super().forward(input)\n",
    "\n",
    "    def train(self, mode: bool = True) -> \"LoRALinear\":\n",
    "        super().train(mode)\n",
    "        if self.is_lora() and self.has_weights_merged:\n",
    "            # Demerge LoRA weights if already merged\n",
    "            self.has_weights_merged = False\n",
    "        return self\n",
    "\n",
    "    def eval(self) -> \"LoRALinear\":\n",
    "        super().eval()\n",
    "        if self.is_lora() and not self.has_weights_merged:\n",
    "            # Merge LoRA weights when switching to evaluation mode\n",
    "            self.has_weights_merged = True\n",
    "        return self\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        out = nn.Linear.extra_repr(self)\n",
    "        if self.is_lora():\n",
    "            out += f', lora_rank={self.lora_A.weight.shape[0]}, lora_scaling={self.lora_scaling}, lora_dropout={self.lora_dropout.p}'\n",
    "        return out\n",
    "\n",
    "def mark_only_lora_as_trainable(model: nn.Module) -> nn.Module:\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    # Loop through all LoRA layers and mark them as trainable\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, LoRALinear) and module.is_lora():\n",
    "            module.lora_A.weight.requires_grad = True\n",
    "            module.lora_B.weight.requires_grad = True\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py31",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
