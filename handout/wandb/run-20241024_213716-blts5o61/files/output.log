ddp False
tokens per iteration will be: 131,072
Positive: 2478, Negative: 2522
loading weights from pretrained gpt: gpt2
forcing vocab_size=50257, block_size=1024, bias=True
overriding dropout rate to 0.0
overriding lora_rank and lora_alpha to 128, 512
overriding lora_dropout to 0.05
number of parameters: 130.73M
torch.Size([128, 768])
torch.Size([2304, 128])
torch.Size([128, 768])
torch.Size([768, 128])
torch.Size([128, 768])
torch.Size([2304, 128])
torch.Size([128, 768])
torch.Size([768, 128])
torch.Size([128, 768])
torch.Size([2304, 128])
torch.Size([128, 768])
torch.Size([768, 128])
torch.Size([128, 768])
torch.Size([2304, 128])
torch.Size([128, 768])
torch.Size([768, 128])
torch.Size([128, 768])
torch.Size([2304, 128])
torch.Size([128, 768])
torch.Size([768, 128])
torch.Size([128, 768])
torch.Size([2304, 128])
torch.Size([128, 768])
torch.Size([768, 128])
torch.Size([128, 768])
torch.Size([2304, 128])
torch.Size([128, 768])
torch.Size([768, 128])
torch.Size([128, 768])
torch.Size([2304, 128])
torch.Size([128, 768])
torch.Size([768, 128])
torch.Size([128, 768])
torch.Size([2304, 128])
torch.Size([128, 768])
torch.Size([768, 128])
torch.Size([128, 768])
torch.Size([2304, 128])
torch.Size([128, 768])
torch.Size([768, 128])
torch.Size([128, 768])
torch.Size([2304, 128])
torch.Size([128, 768])
torch.Size([768, 128])
torch.Size([128, 768])
torch.Size([2304, 128])
torch.Size([128, 768])
torch.Size([768, 128])
num decayed parameter tensors: 48, with 7,077,888 parameters
num non-decayed parameter tensors: 0, with 0 parameters
using fused AdamW: True
C:\Users\NIshanth Mohankumar\OneDrive\Desktop\CMU_Courses\Gen AI\hw3\handout\model.py:92: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\cb\pytorch_1000000000000\work\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:455.)
  y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)
Evaluation at Iter 0: Val Loss 4.9968
Best val loss so far, saving Best Val checkpoint...
iter 0: loss 4.7301, time 14931.71ms, mfu -100.00%
number of parameters: 130.73M
Best Val Checkpoint || Accuracy: 0.02, Positive Predictions: 3, Negative Predictions: 2, Correct Predictions: 2
number of parameters: 130.73M
Traceback (most recent call last):
  File "C:\Users\NIshanth Mohankumar\OneDrive\Desktop\CMU_Courses\Gen AI\hw3\handout\train.py", line 355, in <module>
    main()
  File "C:\Users\NIshanth Mohankumar\OneDrive\Desktop\CMU_Courses\Gen AI\hw3\handout\train.py", line 344, in main
    accuracy, pos_counter, neg_counter, counter = sampler.get_accuracy()
                                                  ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\NIshanth Mohankumar\OneDrive\Desktop\CMU_Courses\Gen AI\hw3\handout\generate.py", line 91, in get_accuracy
    pred_samples = self.predict_labels(self.test_dataset)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\NIshanth Mohankumar\OneDrive\Desktop\CMU_Courses\Gen AI\hw3\handout\generate.py", line 68, in predict_labels
    response = self.get_generation(prompt)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\NIshanth Mohankumar\OneDrive\Desktop\CMU_Courses\Gen AI\hw3\handout\generate.py", line 61, in get_generation
    y = self.model.generate(x, self.max_new_tokens, temperature=self.temperature, top_k=self.top_k)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "E:\Users\Nishanth\envs\py31\Lib\site-packages\torch\utils\_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\NIshanth Mohankumar\OneDrive\Desktop\CMU_Courses\Gen AI\hw3\handout\model.py", line 359, in generate
    logits, _ = self(idx_cond)
                ^^^^^^^^^^^^^^
  File "E:\Users\Nishanth\envs\py31\Lib\site-packages\torch\nn\modules\module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "E:\Users\Nishanth\envs\py31\Lib\site-packages\torch\nn\modules\module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\NIshanth Mohankumar\OneDrive\Desktop\CMU_Courses\Gen AI\hw3\handout\model.py", line 219, in forward
    logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "E:\Users\Nishanth\envs\py31\Lib\site-packages\torch\nn\modules\module.py", line 1528, in _wrapped_call_impl
    def _wrapped_call_impl(self, *args, **kwargs):
KeyboardInterrupt