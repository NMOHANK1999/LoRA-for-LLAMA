ddp False
tokens per iteration will be: 131,072
Positive: 2478, Negative: 2522
loading weights from pretrained gpt: gpt2
forcing vocab_size=50257, block_size=1024, bias=True
overriding dropout rate to 0.0
overriding lora_rank and lora_alpha to 128, 512
overriding lora_dropout to 0.05
> [1mc:\users\nishanth mohankumar\onedrive\desktop\cmu_courses\gen ai\hw3\handout\lora.py[22m(37)__init__[1m()
[1m     36             [22mipdb[1m.[22mset_trace[1m()
[1m---> 37             [22mself[1m.[22mlora_A [1m=[22m nn[1m.[22mLinear[1m([22min_features[1m,[22m lora_rank[1m,[22m bias[1m=[22m bias[1m)
[1m     38             [22mself[1m.[22mlora_B [1m=[22m nn[1m.[22mLinear[1m([22mlora_rank[1m,[22m out_features[1m,[22m bias[1m=[22m bias[1m)
> [1mc:\users\nishanth mohankumar\onedrive\desktop\cmu_courses\gen ai\hw3\handout\lora.py[22m(38)__init__[1m()
[1m     37             [22mself[1m.[22mlora_A [1m=[22m nn[1m.[22mLinear[1m([22min_features[1m,[22m lora_rank[1m,[22m bias[1m=[22m bias[1m)
[1m---> 38             [22mself[1m.[22mlora_B [1m=[22m nn[1m.[22mLinear[1m([22mlora_rank[1m,[22m out_features[1m,[22m bias[1m=[22m bias[1m)
[1m     39 
> [1mc:\users\nishanth mohankumar\onedrive\desktop\cmu_courses\gen ai\hw3\handout\lora.py[22m(40)__init__[1m()
[1m     39 
[1m---> 40             [22mself[1m.[22mlora_A[1m.[22mrequires_grad [1m=[22m [1mFalse
[1m     41             [22mself[1m.[22mlora_B[1m.[22mrequires_grad [1m=[22m [1mFalse
Linear(in_features=768, out_features=128, bias=True)
> [1mc:\users\nishanth mohankumar\onedrive\desktop\cmu_courses\gen ai\hw3\handout\lora.py[22m(41)__init__[1m()
[1m     40             [22mself[1m.[22mlora_A[1m.[22mrequires_grad [1m=[22m [1mFalse
[1m---> 41             [22mself[1m.[22mlora_B[1m.[22mrequires_grad [1m=[22m [1mFalse
[1m     42 
Linear(in_features=768, out_features=128, bias=True)
Parameter containing:
tensor([[ 0.0305,  0.0075, -0.0305,  ...,  0.0247, -0.0337, -0.0293],
        [-0.0026, -0.0148, -0.0234,  ..., -0.0294,  0.0359,  0.0262],
        [-0.0170, -0.0354,  0.0037,  ...,  0.0336, -0.0261, -0.0298],
        ...,
        [-0.0042, -0.0033, -0.0045,  ...,  0.0346,  0.0341,  0.0122],
        [ 0.0151, -0.0206, -0.0170,  ...,  0.0027, -0.0053, -0.0067],
        [ 0.0241,  0.0300, -0.0248,  ..., -0.0186,  0.0184, -0.0144]],
       requires_grad=True)
Linear(in_features=768, out_features=128, bias=True)
Traceback (most recent call last):
  File "C:\Users\NIshanth Mohankumar\OneDrive\Desktop\CMU_Courses\Gen AI\hw3\handout\train.py", line 356, in <module>
    main()
  File "C:\Users\NIshanth Mohankumar\OneDrive\Desktop\CMU_Courses\Gen AI\hw3\handout\train.py", line 217, in main
    model = GPT.from_pretrained(args.init_from, override_args)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\NIshanth Mohankumar\OneDrive\Desktop\CMU_Courses\Gen AI\hw3\handout\model.py", line 271, in from_pretrained
    model = GPT(config)
            ^^^^^^^^^^^
  File "C:\Users\NIshanth Mohankumar\OneDrive\Desktop\CMU_Courses\Gen AI\hw3\handout\model.py", line 160, in __init__
    h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\NIshanth Mohankumar\OneDrive\Desktop\CMU_Courses\Gen AI\hw3\handout\model.py", line 160, in <listcomp>
    h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
                       ^^^^^^^^^^^^^
  File "C:\Users\NIshanth Mohankumar\OneDrive\Desktop\CMU_Courses\Gen AI\hw3\handout\model.py", line 126, in __init__
    self.attn = CausalSelfAttention(config)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\NIshanth Mohankumar\OneDrive\Desktop\CMU_Courses\Gen AI\hw3\handout\model.py", line 49, in __init__
    self.c_attn = LoRALinear(
                  ^^^^^^^^^^^
  File "C:\Users\NIshanth Mohankumar\OneDrive\Desktop\CMU_Courses\Gen AI\hw3\handout\lora.py", line 41, in __init__
    self.lora_B.requires_grad = False
                                ^^^^^
  File "E:\Users\Nishanth\envs\py31\Lib\bdb.py", line 90, in trace_dispatch
    return self.dispatch_line(frame)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "E:\Users\Nishanth\envs\py31\Lib\bdb.py", line 115, in dispatch_line
    if self.quitting: raise BdbQuit
                      ^^^^^^^^^^^^^
bdb.BdbQuit
If you suspect this is an IPython 8.27.0 bug, please report it at:
    https://github.com/ipython/ipython/issues
or send an email to the mailing list at ipython-dev@python.org
You can print a more detailed traceback right now with "%tb", or use "%debug"
to interactively debug it.
Extra-detailed tracebacks for bug-reporting purposes can be enabled via:
    %config Application.verbose_crash=True