ddp False
tokens per iteration will be: 131,072
Positive: 2478, Negative: 2522
loading weights from pretrained gpt: gpt2
forcing vocab_size=50257, block_size=1024, bias=True
overriding dropout rate to 0.0
overriding lora_rank and lora_alpha to 128, 512
overriding lora_dropout to 0.05
number of parameters: 130.73M
num decayed parameter tensors: 48, with 7,077,888 parameters
num non-decayed parameter tensors: 0, with 0 parameters
using fused AdamW: True
C:\Users\NIshanth Mohankumar\OneDrive\Desktop\CMU_Courses\Gen AI\hw3\handout\model.py:92: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\cb\pytorch_1000000000000\work\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:455.)
  y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)
Evaluation at Iter 0: Val Loss 4.9968
Best val loss so far, saving Best Val checkpoint...
iter 0: loss 4.7301, time 14234.47ms, mfu -100.00%
number of parameters: 130.73M
Traceback (most recent call last):
  File "C:\Users\NIshanth Mohankumar\OneDrive\Desktop\CMU_Courses\Gen AI\hw3\handout\train.py", line 355, in <module>
    main()
  File "C:\Users\NIshanth Mohankumar\OneDrive\Desktop\CMU_Courses\Gen AI\hw3\handout\train.py", line 335, in main
    accuracy, pos_counter, neg_counter, counter = sampler.get_accuracy()
                                                  ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\NIshanth Mohankumar\OneDrive\Desktop\CMU_Courses\Gen AI\hw3\handout\generate.py", line 94, in get_accuracy
    pred_samples = self.predict_labels(self.test_dataset)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\NIshanth Mohankumar\OneDrive\Desktop\CMU_Courses\Gen AI\hw3\handout\generate.py", line 68, in predict_labels
    prompt = dataloader.get_sentiment_prompt(row["text"])
             ^^^^^^^^^^
NameError: name 'dataloader' is not defined